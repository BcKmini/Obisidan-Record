## ğŸš€ ë…¼ë¬¸

ì´ ë¸”ë¡œê·¸ì—ì„œëŠ” **NLP ë…¼ë¬¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ì½ëŠ” ë°©ë²•ê³¼ ì¶”ì²œ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸**ë¥¼ ì •ë¦¬  
ğŸ’¡ **Tip!** ì²˜ìŒë¶€í„° ìµœì‹  ë…¼ë¬¸ì„ ì½ê¸°ë³´ë‹¤ëŠ”, **ê¸°ì´ˆ â†’ ì „í†µ ë…¼ë¬¸ â†’ ìµœì‹  ë…¼ë¬¸** ìˆœ

---

## ğŸ“Œ ë…¼ë¬¸ ì½ê¸° ì¶”ì²œ ìˆœì„œ 

> âœ¨ **ëª©í‘œ**: ë‹¨ìˆœíˆ ë…¼ë¬¸ì„ ì½ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ë…¼ë¬¸ ì† ê°œë…ì„ ì´í•´í•˜ê³  ì ìš©í•˜ëŠ” ê²ƒ!**

### 1ï¸âƒ£ ê¸°ì´ˆ ê°œë… ë‹¤ì§€ê¸° (Word Embedding & RNN)

NLPë¥¼ ì²˜ìŒ ê³µë¶€í•œë‹¤ë©´, ë¨¼ì € **ë‹¨ì–´ë¥¼ ì–´ë–»ê²Œ ìˆ˜ì¹˜ë¡œ ë³€í™˜í•˜ëŠ”ì§€** ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•´ìš”.  
ğŸ’¡ **ì¶”ì²œ ë…¼ë¬¸ ë° ìë£Œ**

- **Word2Vec**: [Efficient Estimation of Word Representations in Vector Space (2013)](https://arxiv.org/abs/1301.3781)
- **GloVe**: [Global Vectors for Word Representation (2014)](https://nlp.stanford.edu/pubs/glove.pdf)
- **FastText**: [Enriching Word Vectors with Subword Information (2016)](https://arxiv.org/abs/1607.04606)
- ğŸ“– **ì¶”ì²œ ë¸”ë¡œê·¸**: [Word Embedding ì •ë¦¬](https://asidefine.tistory.com/152)

ë‹¤ìŒìœ¼ë¡œ, NLPì˜ ê¸°ë³¸ êµ¬ì¡°ì¸ RNNê³¼ LSTMì„ ì´í•´í•´ì•¼ í•´ìš”.

- **RNN**: [Recurrent Neural Network based language model (2010)](https://arxiv.org/abs/1409.3215)
- **LSTM**: Long Short Term Memory (1997, Hochreiter & Schmidhuber)
- **GRU**: [Learning Phrase Representations using RNN Encoder-Decoder (2014)](https://arxiv.org/abs/1406.1078)

ğŸ“Œ **ì´ ë‹¨ê³„ì—ì„œ í•  ì¼**

- Word2Vec, GloVe, FastText êµ¬í˜„í•´ë³´ê¸°
- LSTMê³¼ GRUë¥¼ ë¹„êµí•˜ë©´ì„œ í•™ìŠµ

---

### 2ï¸âƒ£ Attention & Transformer ì´í•´í•˜ê¸°

**"Attention is All You Need" (2017)** ë…¼ë¬¸ì´ ë“±ì¥í•˜ë©´ì„œ NLPì˜ íŒ¨ëŸ¬ë‹¤ì„ì´ ì™„ì „íˆ ë°”ë€Œì—ˆì–´ìš”.  
ğŸ’¡ **ì¶”ì²œ ë…¼ë¬¸**

- **Seq2Seq**: [Sequence to Sequence Learning with Neural Networks (2014)](https://arxiv.org/abs/1409.3215)
- **Attention Mechanism**: [Neural Machine Translation by Jointly Learning to Align and Translate (2015)](https://arxiv.org/abs/1409.0473)
- **Transformer**: [Attention is All You Need (2017)](https://arxiv.org/abs/1706.03762)
- ğŸ“– **ì¶”ì²œ ë¸”ë¡œê·¸**: [Attention & Transformer ì •ë¦¬](https://asidefine.tistory.com/153)

ğŸ“Œ **ì´ ë‹¨ê³„ì—ì„œ í•  ì¼**

- Transformer êµ¬ì¡° ì§ì ‘ êµ¬í˜„í•´ë³´ê¸° (TensorFlow/PyTorch í™œìš©)
- Seq2Seqì™€ Transformer ë¹„êµí•´ë³´ê¸°

---

### 3ï¸âƒ£ Pretrained Language Models (PLMs)

Transformer ê¸°ë°˜ì˜ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸(Pretrained Language Model)ì´ NLPë¥¼ í˜ì‹ ì ìœ¼ë¡œ ë°œì „ì‹œì¼°ì–´ìš”!  
ğŸ’¡ **ì¶”ì²œ ë…¼ë¬¸**

- **BERT**: [Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)](https://arxiv.org/abs/1810.04805)
- **GPT-1**: Improving Language Understanding by Generative Pre-Training (2018)
- **GPT-2**: [Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- **GPT-3**: [Language Models are Few-Shot Learners (2020)](https://arxiv.org/abs/2005.14165)
- **RoBERTa**: [A Robustly Optimized BERT Pretraining Approach (2019)](https://arxiv.org/abs/1907.11692)
- **ELECTRA**: [Pre-training Text Encoders as Discriminators Rather Than Generators (2020)](https://arxiv.org/abs/2003.10555)
- **T5**: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019)](https://arxiv.org/abs/1910.10683)

ğŸ“Œ **ì´ ë‹¨ê³„ì—ì„œ í•  ì¼**

- Hugging Face `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ PLMì„ fine-tuning í•´ë³´ê¸°
- BERT vs GPT ëª¨ë¸ ë¹„êµ

---

### 4ï¸âƒ£ ìµœì‹  NLP ë…¼ë¬¸ ì½ê¸°

ìµœê·¼ì—ëŠ” **LLM (Large Language Model)** ê³¼ **RAG (Retrieval-Augmented Generation)** ê°€ ê°€ì¥ í•«í•œ ì—°êµ¬ ì£¼ì œì˜ˆìš”!  
ğŸ’¡ **ì¶”ì²œ ë…¼ë¬¸**

- **LaMDA**: [Language Models for Dialog Applications (2022)](https://arxiv.org/abs/2201.08239)
- **PaLM**: [Scaling Language Modeling with Pathways (2022)](https://arxiv.org/abs/2204.02311)
- **ChatGPT & GPT-4**: [GPT-4 Technical Report (2023)](https://arxiv.org/abs/2303.08774)
- **LLaMA**: [Open and Efficient Foundation Language Models (2023)](https://arxiv.org/abs/2302.13971)
- **RAG**: [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (2020)](https://arxiv.org/abs/2005.11401)

ğŸ“Œ  **ì´ ë‹¨ê³„ì—ì„œ í•  ì¼**

- LLM ëª¨ë¸ì„ ì‚¬ìš©í•´ ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ ì§„í–‰
- ìµœì‹  ë…¼ë¬¸ì˜ êµ¬í˜„ ì½”ë“œ ì°¾ì•„ë³´ê¸°

---


> NLP ê³µë¶€ ì°¸ê³  ë¸”ë¡œê·¸
https://jonhyuk0922.tistory.com/169
https://asidefine.tistory.com/180


> ì¶”ì²œë“œë¦¬ëŠ” ìœ„í‚¤ë…ìŠ¤, ê¹ƒë¶ í˜ì´ì§€
- https://wikidocs.net/book/2788 (ë”¥ëŸ¬ë‹)  
- https://wikidocs.net/book/2155 (ìì—°ì–´ì²˜ë¦¬)  
- https://kh-kim.github.io/nlp_with_deep_learning_blog/ (ìì—°ì–´ì²˜ë¦¬)  
- https://wikidocs.net/book/8056 (Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬, ë§¨ ë§ˆì§€ë§‰ì— ê³µë¶€í•  ê²ƒ)  
- https://tutorials.pytorch.kr/beginner/text_sentiment_ngrams_tutorial.html (íŒŒì´í† ì¹˜ ê³µì‹ ë„íë¨¼íŠ¸)

- [ìì—°ì–´ ì²˜ë¦¬](https://wikidocs.net/21693)

---
d
