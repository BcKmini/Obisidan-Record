# 📌 [ML](https://wikidocs.net/book/5942)용어 공부
# loss function (손실 함수), cost function (비용 함수)

손실 함수(Loss Function)는 [기계 학습](https://wikidocs.net/120136)과 [딥러닝](https://wikidocs.net/120131)에서 모델의 성능을 측정하는 데 사용되는 함수이다. 이 함수는 모델의 예측값과 실제값 간의 차이를 수치화하여, 모델이 얼마나 잘 또는 못하고 있는지를 나타낸다. 손실 함수의 값이 낮을수록 모델의 성능이 더 좋다고 평가된다. 기계 학습 모델의 학습 과정은 이 손실 함수를 최소화하는 방향으로 진행된다. 손실 함수의 선택은 해결하려는 문제의 유형([회귀](https://wikidocs.net/120111), [분류](https://wikidocs.net/120197), [클러스터링](https://wikidocs.net/120176) 등)에 따라 달라진다.

일반적으로 사용되는 손실 함수에는 [평균 제곱 오차(Mean Squared Error, MSE)](https://wikidocs.net/170549), [교차 엔트로피(Cross-Entropy)](https://wikidocs.net/157190), [힌지 손실(Hinge Loss)](https://en.wikipedia.org/wiki/Hinge_loss) 등이 있다. MSE는 주로 회귀 문제에서 사용되며, 교차 엔트로피는 분류 문제에 자주 사용된다. 손실 함수는 모델의 학습 방향을 결정하는 중요한 요소로, 모델의 목표와 데이터 특성에 적합한 함수를 선택하는 것이 중요하다.

# earning rate (학습률)

학습률(learning rate)은[](https://wikidocs.net/126061#)

 [인공](https://wikidocs.net/126061#) 신경망과 같은 기계 학습 모델이 얼마나 빠르게 학습하는지를 결정하는 [하이퍼파라미터](https://wikidocs.net/120048)이다. 이는 최적화 알고리듬에서 [손실 함수](https://wikidocs.net/120077)의 최소값을 찾아가는 과정에서 각 반복(iteration) 당 이동하는 걸음의 크기를 조정한다. 학습률의 값은 작은 양수로, 너무 높으면 알고리즘이 최소점을 지나치게 되며, 너무 낮으면 수렴하는데 오래 걸리거나 원하지 않는 지역 최소점(local minimum)에 빠질 수 있다.

학습률은 일반적으로 학습 초기에는 높게 설정하여 빠른 학습을 가능하게 하고, 점차 줄여나가는 방식으로 조정된다. 이러한 조정은 학습률 스케줄(learning rate schedule) 또는 적응형 학습률(adaptive learning rate) 방식을 통해 이루어진다. 적응형 학습률은 모델이 데이터를 통해 학습할 때 발생하는 오류를 기반으로 학습률을 동적으로 조정하여, 학습 과정을 최적화한다.

학습률의 적절한 설정은 신경망의 성능을 크게 좌우할 수 있으며, 이는 [스토캐스틱 경사 하강법(Stochastic Gradient Descent, SGD)](https://wikidocs.net/120078)과 같은 최적화 기법에 의해 구현된다. 이러한 기법은 각 데이터 포인트 또는 데이터 배치를 사용하여 모델의 가중치를 업데이트함으로써 학습을 진행한다. 이 과정에서 학습률이 너무 높거나 낮은 경우, 모델이 최적의 학습 결과를 도출하지 못할 위험이 있어, 실험을 통해 최적의 값을 찾는 것이 필요하다.


> 참고
>- Deepchecks. (n.d.). What is Learning Rate in[](https://wikidocs.net/126061#)[Machine Learning](https://wikidocs.net/126061#)? 
>- Retrieved from [https://deepchecks.com/glossary/learning-rate-in-machine-learning/](https://deepchecks.com/glossary/learning-rate-in-machine-learning/)
>- Wikipedia. (n.d.). Learning rate. Retrieved from [https://en.wikipedia.org/wiki/Learning_rate](https://en.wikipedia.org/wiki/Learning_rate)
>- Machine Learning Mastery. (n.d.). Understand the Impact of Learning Rate on Neural Network Performance. Retrieved from [https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)
>- Study Machine Learning. (n.d.). Neural Network: Introduction to Learning Rate. Retrieved from [https://studymachinelearning.com/neural-network-introduction-to-learning-rate/](https://studymachinelearning.com/neural-network-introduction-to-learning-rate/)

---
# **📌 Learning Rate와 Batch Size 관계 정리**
![](https://blog.kakaocdn.net/dn/vHd5n/btszRUdVDHz/4W9MyzV6k6IbzgeSiK1WX1/img.png)

딥러닝에서 가장 중요한 하이퍼파라미터는 **Learning Rate (LR)**과 **Batch Size (BS)**입니다.  
특히 **큰 모델과 데이터셋**을 다룰 때, 적절한 조합을 찾는 것은 필수적이다.  
- LR과 BS의 관계를 이해하고, 최적의 값을 찾는 방법을 정리
---
![](https://blog.kakaocdn.net/dn/VzCpN/btszUwpUyvb/M1jo68Y7WYGVuiMWIo2n80/img.png)
## **1. Learning Rate 조정 방법**

일반적으로 LR을 찾을 때는 다음 방법을 사용합니다.

1. **고정된 Epoch, Batch Size를 설정**
2. **LR을 여러 배수로 변화시키며 Loss를 비교**
    - $$
    - LR=x1,x2,x4,x1/2,x1/4
    - $$값으로 실험
3. **Loss 그래프를 보고 적절한 LR 선택**

작은 CNN 모델과 작은 데이터셋(CIFAR-10 등)에서는 위 방식으로 충분하다.  
하지만 **대형 모델 (Transformer, ResNet, ViT 등)과 대형 데이터셋 (ImageNet 등)**을 학습할 때는 문제가 발생한다.

---

## **2. 큰 모델을 학습할 때 문제점**

대형 모델을 학습할 때 가장 큰 문제는 **컴퓨팅 리소스 부족**입니다.  
논문에서 **Batch Size = 1024**로 학습했다고 하더라도,  
우리는 **Batch Size = 64**와 같은 작은 값으로 학습해야 할 수 있습니다.

이때 LR을 그대로 유지하면 문제가 발생합니다.  
따라서 **Batch Size와 Learning Rate의 관계**를 이해해야 합니다.

---

## **3. Batch Size와 Learning Rate 관계**

### **1) Mini-Batch Gradient Descent**

미니배치 학습에서는 각 배치마다 Loss를 계산하고 이를 역전파합니다.  
즉, **Batch Size가 줄어들면 같은 Epoch에서도 Update 횟수가 증가**합니다.

예를 들어,

- **Batch Size = 1024, Learning Rate = 16**
- **Batch Size = 64** 로 줄이면?
    - 학습 횟수가 1024/64=161024/64 = 161024/64=16배 증가
    - 따라서 **Learning Rate도 16배 줄여야 비슷한 효과를 얻음**
    - 즉, LR=16→LR=1LR = 16 \to LR = 1LR=16→LR=1

✔ **Batch Size 변화 비율에 맞춰 Learning Rate도 조정해야 한다.**

$$
New Learning Rate=Original Learning Rate×Original Batch SizeNew Batch Size​
$$

하지만 이렇게 조정한다고 **정확히 동일한 결과가 나오지는 않는다**.  
이제 그 이유를 살펴보자.

---
![img.png](https://blog.kakaocdn.net/dn/bZBUWS/btsz6jq6EwP/TooQiQxzgkrQmAAwLyjsV0/img.png)
### **📌 Learning Rate와 Batch Size의 특성 비교**

| **항목**            | **큰 값 (High/Big)**                                           | **작은 값 (Low/Small)**                                                 |
| ----------------- | ------------------------------------------------------------ | -------------------------------------------------------------------- |
| **Learning Rate** | Local Minimum에 덜 갇힘  <br>발산 위험 있음  <br>학습 속도가 빠름             | 발산 위험 없음  <br>Local Minimum에 갇힐 위험  <br>학습 속도가 느림                    |
| **Batch Size**    | 최적값 탐색이 안정적 (진동 적음)  Local Minimum에 빠질 위험  <br>병렬 연산 가능 → 빠름 | 특정 데이터가 영향을 줘 Local Minimum 탈출 가능  <br>진동이 심해 불안정  <br>병렬 연산 제한 → 느림 |
| **조합 효과**         | **High LR + Big BS** → 빠르고 안정적  <br>(But, Local Minimum 위험)  | **Low LR + Small BS** → Local Minimum 탈출 가능  <br>(But, 속도가 느림)       |

---
![](https://blog.kakaocdn.net/dn/VvtUw/btsz6kKgUbQ/KyFIiwgyaYNbsfb4rSqJ11/img.png)
### **📌 Batch Size를 줄일 때 Learning Rate 조정 방법**

| **원래 설정 (논문 기준)**                  | **Batch Size 줄인 경우**                              | **추가 조정 필요 여부**      |
| ---------------------------------- | ------------------------------------------------- | ------------------- |
| **BS = 1024, LR = 16, Epoch = 10** | **BS = 64, LR = 1, Epoch = 10**                   | 추가 조정                |
| **Case 1: Accuracy ↓ (성능 저하 발생)**  | **BS = 64, LR = 1, Epoch = 12** (+Early Stopping) | Epoch 증가             |
| **Case 2: Overfitting 발생**         | **BS = 64, LR = 0.8, Epoch = 10**               Learning Rate 감소 필요 소 필요 |

---

### **📌 Learning Rate & Batch Size 조정 공식**
 
 **Batch Size 변화 비율에 맞춰 Learning Rate도 조정해야 함**

$$
New LR=Old LR×Old BSNew BS​
$$

하지만, 단순 환산만으로 같은 성능이 보장되지 않음  
**Loss 그래프 분석 후 추가적인 하이퍼파라미터 튜닝 필요**

---

## **5. Batch Size 줄일 때 추가적인 조정 필요**

**Batch Size를 줄인다고 해서 단순히 LR만 바꾸면 동일한 결과를 보장할 수 없습니다.**  
보완책이 필요합니다.

### **Case 1: Accuracy가 미세하게 낮음**

- **조치:** Epoch를 10% 정도 증가
- **예시:**
    $$
    - 논문: BS=1024,LR=16,Epoch=10BS = 1024, LR = 16, Epoch = 10BS=1024,LR=16,Epoch=10
    - $$ $$
- 구현: BS=64,LR=1,Epoch=10BS = 64, LR = 1, Epoch = 10BS=64,LR=1,Epoch=10
- $$$$
- 해결: BS=64,LR=1,Epoch=12BS = 64, LR = 1, Epoch = 12BS=64,LR=1,Epoch=12 (+ Early Stopping)
$$

### **Case 2: 과적합 발생**

- **조치:** Learning Rate를 더 낮춤
- **예시:**$$
    - 논문: BS=1024,LR=16,Epoch=10BS = 1024, LR = 16, Epoch = 10BS=1024,LR=16,Epoch=10
    - $$$$- 구현: BS=64,LR=1,Epoch=10BS = 64, LR = 1, Epoch = 10BS=64,LR=1,Epoch=10$$$$- 해결: BS=64,LR=0.8,Epoch=10BS = 64, LR = 0.8, Epoch = 10BS=64,LR=0.8,Epoch=10 $$

---

## **6. 정리: 좋은 Learning Rate를 찾는 방법**

4. **Batch Size를 줄이면 Learning Rate도 줄여야 함**
$$
New LR=Old LR×Old BSNew BS​
$$
1. **Batch Size 감소 시 추가 조정 필요**
    - Case 1: Accuracy ↓ → Epoch 증가
    - Case 2: Overfitting → LR 감소
2. **Loss 그래프를 보면서 최적값 찾기**
$$
LR=x1,x2,x4,x1/2,x1/4
$$로 실험 후 비교

✔ **Batch Size 줄이면 반드시 Hyperparameter 튜닝을 다시 해야 한다.**  
✔ **Epoch와 Learning Rate를 조정하며 최적의 조합을 찾아야 한다.**

---

## **7. 최종 요약**

**Batch Size 증가 → Learning Rate 증가 가능 (더 빠르고 안정적)**  
**Batch Size 감소 → Learning Rate 감소 필수 (안정성 유지)**  
**하지만 정확히 같은 성능을 내기 위해서는 추가적인 조정이 필요**   
**Loss 그래프를 참고하며 실험적으로 최적값을 찾아야 한다.**

---

