# 📌 [ML](https://wikidocs.net/book/5942)용어 공부
# loss function (손실 함수), cost function (비용 함수)

손실 함수(Loss Function)는 [기계 학습](https://wikidocs.net/120136)과 [딥러닝](https://wikidocs.net/120131)에서 모델의 성능을 측정하는 데 사용되는 함수이다. 이 함수는 모델의 예측값과 실제값 간의 차이를 수치화하여, 모델이 얼마나 잘 또는 못하고 있는지를 나타낸다. 손실 함수의 값이 낮을수록 모델의 성능이 더 좋다고 평가된다. 기계 학습 모델의 학습 과정은 이 손실 함수를 최소화하는 방향으로 진행된다. 손실 함수의 선택은 해결하려는 문제의 유형([회귀](https://wikidocs.net/120111), [분류](https://wikidocs.net/120197), [클러스터링](https://wikidocs.net/120176) 등)에 따라 달라진다.

일반적으로 사용되는 손실 함수에는 [평균 제곱 오차(Mean Squared Error, MSE)](https://wikidocs.net/170549), [교차 엔트로피(Cross-Entropy)](https://wikidocs.net/157190), [힌지 손실(Hinge Loss)](https://en.wikipedia.org/wiki/Hinge_loss) 등이 있다. MSE는 주로 회귀 문제에서 사용되며, 교차 엔트로피는 분류 문제에 자주 사용된다. 손실 함수는 모델의 학습 방향을 결정하는 중요한 요소로, 모델의 목표와 데이터 특성에 적합한 함수를 선택하는 것이 중요하다.

# earning rate (학습률)

학습률(learning rate)은[](https://wikidocs.net/126061#)

 [인공](https://wikidocs.net/126061#) 신경망과 같은 기계 학습 모델이 얼마나 빠르게 학습하는지를 결정하는 [하이퍼파라미터](https://wikidocs.net/120048)이다. 이는 최적화 알고리듬에서 [손실 함수](https://wikidocs.net/120077)의 최소값을 찾아가는 과정에서 각 반복(iteration) 당 이동하는 걸음의 크기를 조정한다. 학습률의 값은 작은 양수로, 너무 높으면 알고리즘이 최소점을 지나치게 되며, 너무 낮으면 수렴하는데 오래 걸리거나 원하지 않는 지역 최소점(local minimum)에 빠질 수 있다.

학습률은 일반적으로 학습 초기에는 높게 설정하여 빠른 학습을 가능하게 하고, 점차 줄여나가는 방식으로 조정된다. 이러한 조정은 학습률 스케줄(learning rate schedule) 또는 적응형 학습률(adaptive learning rate) 방식을 통해 이루어진다. 적응형 학습률은 모델이 데이터를 통해 학습할 때 발생하는 오류를 기반으로 학습률을 동적으로 조정하여, 학습 과정을 최적화한다.

학습률의 적절한 설정은 신경망의 성능을 크게 좌우할 수 있으며, 이는 [스토캐스틱 경사 하강법(Stochastic Gradient Descent, SGD)](https://wikidocs.net/120078)과 같은 최적화 기법에 의해 구현된다. 이러한 기법은 각 데이터 포인트 또는 데이터 배치를 사용하여 모델의 가중치를 업데이트함으로써 학습을 진행한다. 이 과정에서 학습률이 너무 높거나 낮은 경우, 모델이 최적의 학습 결과를 도출하지 못할 위험이 있어, 실험을 통해 최적의 값을 찾는 것이 필요하다.


> 참고
>- Deepchecks. (n.d.). What is Learning Rate in[](https://wikidocs.net/126061#)[Machine Learning](https://wikidocs.net/126061#)? 
>- Retrieved from [https://deepchecks.com/glossary/learning-rate-in-machine-learning/](https://deepchecks.com/glossary/learning-rate-in-machine-learning/)
>- Wikipedia. (n.d.). Learning rate. Retrieved from [https://en.wikipedia.org/wiki/Learning_rate](https://en.wikipedia.org/wiki/Learning_rate)
>- Machine Learning Mastery. (n.d.). Understand the Impact of Learning Rate on Neural Network Performance. Retrieved from [https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)
>- Study Machine Learning. (n.d.). Neural Network: Introduction to Learning Rate. Retrieved from [https://studymachinelearning.com/neural-network-introduction-to-learning-rate/](https://studymachinelearning.com/neural-network-introduction-to-learning-rate/)


이론

# learning rate와 batch size 관계

by davidlds 2023. 11. 7.

![](https://blog.kakaocdn.net/dn/vHd5n/btszRUdVDHz/4W9MyzV6k6IbzgeSiK1WX1/img.png)

good learning rate

딥러닝 모델로 학습을 하면서 가장 많이 겪는 문제점은 hyperparameter 문제다.

learning rate와 epoch를 어떻게 결정하는지가 가장 문제다.

이 문제는 어떤 천재에게 물어도 같은 답이 나온다.

"노가다 말곤 방법이 없어요."

오늘은 그중에서 먼저 learning rate에 대하여 고찰해보자.

일단 간단하게 설명하면 epoch와 batch size를 고정한뒤,

위 그래프를 참고해서 적절한 good learning rate를 찾으면 된다.

그래프를 다 그려가며 비교해보고 좋은 learning rate를 고르자.

즉 learning rate를 x1, x2, x4, x1/2, x1/4 로 학습하며 loss를 그린 뒤에

위 그림을 참고하며 learning rate를 고르면 된다.

CNN이나 작은 데이터셋을 학습할 때에는 이렇게만 하면 된다.

![](https://blog.kakaocdn.net/dn/VzCpN/btszUwpUyvb/M1jo68Y7WYGVuiMWIo2n80/img.png)

늘어나는 모델 사이즈

하지만?

언제까지나 CNN만 학습할 수는 없다. 겁나 큰 Transformer가 기다리고 있다....

언제까지나 CIFAR-10이나 학습할 수는 없다. 겁나 큰 ImageNet이 기다리고 있다....

이런걸 학습할 때 문제가 하나 생긴다.

바로 '똥컴 이슈' 이다.

당신이 아무리 맥북을 쓴다고 해도 아무리 개쩌는 데탑을 쓴다고 해도

연구자들이 사용하는 GPU를 따라잡을 수 없다.

구글 클라우드에서 빌려서 코랩으로 돌릴 수 있는 부자라면 괜찮지만,

최소한 나는 아니다 ㅠㅠ....

논문에서 batch size = 1024 와 같이 거대하게 돌렸는데

나는 batch size = 64 와 같이 귀엽게 돌려야 하는 순간이 찾아온다.

이때 우리는 batch size와 learning rate에 관계가 있다는 걸 명심해야 한다.

![](https://blog.kakaocdn.net/dn/U0oT0/btszOOrATqx/t8txWnIrydcZIrz9kKfKF0/img.png)

mini batch

학습 과정에서 loss function으로 해당 mini batch의 loss를 계산해서 그걸 역전파 시킨다.

그말은 batch size가 반으로 줄어들면 learning rate가 2배로 커지는 효과가 난다는 것이다.

왜냐면 배치가 줄어들면 횟수가 2배로 늘어나기 때문이다.

(1개 * (learning rate) = loss) * 1회

-> (1/2개 * (learning rate) = loss) * 2회

그래서 learning rate를 절반으로 줄여주는 것이 필요하다.

정확히는 batch size의 비율 변화를 learning rate에도 동일하게 적용해야 한다.

(예시 : batch size = 1024, lr = 16 -> batch size = 64, lr = 1)

그렇다면 이렇게 보정하고 학습 했을 때 정 확 하 게 동일한 결과가 나올까?

불행하게도 그 답은... '아닐 수도 있다'

이게.......... 상당히 골머리를 썩는 부분이다.

이제 그 이유에 대하여 살펴보자.

여러가지를 설명하면 머리가 복잡하니, 단순하게 장단점만 생각해보자.

다음은 어느정도 보편적인 사실이다. 나를 믿고 따라오자.

![](https://blog.kakaocdn.net/dn/bZBUWS/btsz6jq6EwP/TooQiQxzgkrQmAAwLyjsV0/img.png)
[learning rate]

(high)

장점 : local minimum에 빠질 위험이 적다.

단점 : 너무 빨라 발산할 수 있다.

(+장점 : 빠르다.)

(low)

장점 : 발산하지 않는다.

단점 : local minimum에 빠질 위험이 있다.

![](https://blog.kakaocdn.net/dn/VvtUw/btsz6kKgUbQ/KyFIiwgyaYNbsfb4rSqJ11/img.png)

batch size

[batch size]

(big)

장점 : 더 많은 데이터를 반영하므로 최적값 탐색 경로가 크게 진동하지 않는다. (발산 가능성 X)

단점 : local minimum에 빠질 위험이 있다. (local을 빠져나올 수 있는 특정 데이터 값이 무시된다.)

(+장점 : 더 많은 병렬 연산을 하므로, 빠르고 효율적이다.)

(small)

장점 : local minimum에 빠질 위험이 적다. (local을 빠져나올 수 있는 특정 데이터 값으로 단번에 탈출한다.)

단점 : 적은 데이터를 반영하므로 최적값 탐색 경로가 크게 진동하며 진행된다. (발산 가능성 O)

이렇게 봤을때

high & big 조합은 서로 상충되는 부분 외에도 이점을 더 얻는다.

low & small 조합에 비하여 **high & big 조합은 상대적으로 더 빠르고 효율적이다.**

그리고 데이터셋의 분산에 따라서 learning rate와 batch size 변화에 민감하게 반응할 수 있다.

그래서 운이 나쁜 경우에는 논문을 정석적으로 환산한 값이 아니라,

case1, accuracy가 미세하게 낮게 나와서 값을 조정하는 번거로움

case2, 과적합 때문에 값을 조정하는 번거로움

두가지 고민을 하게 된다.

ref : (batch size = 1024, lr = 16, epoch = 10)

-> 구현 : (batch size = 64, lr = 1, epoch = 10)

-> case1 : (batch size = 64, lr = 1, epoch = 12) (+early stop)

-> case2 : (batch size = 64, lr = 0.8, epoch = 10)

# case1 에서는 epoch를 늘리는게 리스크가 적다.

# case2 에서는 lr를 줄여야만 더 깊은 optimal minimum을 찾을 수 있다.

이처럼 batch size를 줄이는 그 순간 노가다를 한번 더 해야한다.

100% 똑같지 않으면 해보기 전까지 모르기 때문이다.

epoch를 10% 정도 늘려보고, learning rate를 조금 낮춰보고 비교해야한다.