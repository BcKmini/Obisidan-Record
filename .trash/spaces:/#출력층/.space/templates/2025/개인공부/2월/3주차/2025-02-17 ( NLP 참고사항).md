## 🚀 논문

이 블로그에서는 **NLP 논문을 효과적으로 읽는 방법과 추천 논문 리스트**를 정리  
💡 **Tip!** 처음부터 최신 논문을 읽기보다는, **기초 → 전통 논문 → 최신 논문** 순

---

## 📌 논문 읽기 추천 순서 

> ✨ **목표**: 단순히 논문을 읽는 것이 아니라, **논문 속 개념을 이해하고 적용하는 것!**

### 1️⃣ 기초 개념 다지기 (Word Embedding & RNN)

NLP를 처음 공부한다면, 먼저 **단어를 어떻게 수치로 변환하는지** 이해하는 것이 중요해요.  
💡 **추천 논문 및 자료**

- **Word2Vec**: [Efficient Estimation of Word Representations in Vector Space (2013)](https://arxiv.org/abs/1301.3781)
- **GloVe**: [Global Vectors for Word Representation (2014)](https://nlp.stanford.edu/pubs/glove.pdf)
- **FastText**: [Enriching Word Vectors with Subword Information (2016)](https://arxiv.org/abs/1607.04606)
- 📖 **추천 블로그**: [Word Embedding 정리](https://asidefine.tistory.com/152)

다음으로, NLP의 기본 구조인 RNN과 LSTM을 이해해야 해요.

- **RNN**: [Recurrent Neural Network based language model (2010)](https://arxiv.org/abs/1409.3215)
- **LSTM**: Long Short Term Memory (1997, Hochreiter & Schmidhuber)
- **GRU**: [Learning Phrase Representations using RNN Encoder-Decoder (2014)](https://arxiv.org/abs/1406.1078)

📌 **이 단계에서 할 일**

- Word2Vec, GloVe, FastText 구현해보기
- LSTM과 GRU를 비교하면서 학습

---

### 2️⃣ Attention & Transformer 이해하기

**"Attention is All You Need" (2017)** 논문이 등장하면서 NLP의 패러다임이 완전히 바뀌었어요.  
💡 **추천 논문**

- **Seq2Seq**: [Sequence to Sequence Learning with Neural Networks (2014)](https://arxiv.org/abs/1409.3215)
- **Attention Mechanism**: [Neural Machine Translation by Jointly Learning to Align and Translate (2015)](https://arxiv.org/abs/1409.0473)
- **Transformer**: [Attention is All You Need (2017)](https://arxiv.org/abs/1706.03762)
- 📖 **추천 블로그**: [Attention & Transformer 정리](https://asidefine.tistory.com/153)

📌 **이 단계에서 할 일**

- Transformer 구조 직접 구현해보기 (TensorFlow/PyTorch 활용)
- Seq2Seq와 Transformer 비교해보기

---

### 3️⃣ Pretrained Language Models (PLMs)

Transformer 기반의 사전 훈련 모델(Pretrained Language Model)이 NLP를 혁신적으로 발전시켰어요!  
💡 **추천 논문**

- **BERT**: [Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)](https://arxiv.org/abs/1810.04805)
- **GPT-1**: Improving Language Understanding by Generative Pre-Training (2018)
- **GPT-2**: [Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- **GPT-3**: [Language Models are Few-Shot Learners (2020)](https://arxiv.org/abs/2005.14165)
- **RoBERTa**: [A Robustly Optimized BERT Pretraining Approach (2019)](https://arxiv.org/abs/1907.11692)
- **ELECTRA**: [Pre-training Text Encoders as Discriminators Rather Than Generators (2020)](https://arxiv.org/abs/2003.10555)
- **T5**: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019)](https://arxiv.org/abs/1910.10683)

📌 **이 단계에서 할 일**

- Hugging Face `transformers` 라이브러리를 활용해 PLM을 fine-tuning 해보기
- BERT vs GPT 모델 비교

---

### 4️⃣ 최신 NLP 논문 읽기

최근에는 **LLM (Large Language Model)** 과 **RAG (Retrieval-Augmented Generation)** 가 가장 핫한 연구 주제예요!  
💡 **추천 논문**

- **LaMDA**: [Language Models for Dialog Applications (2022)](https://arxiv.org/abs/2201.08239)
- **PaLM**: [Scaling Language Modeling with Pathways (2022)](https://arxiv.org/abs/2204.02311)
- **ChatGPT & GPT-4**: [GPT-4 Technical Report (2023)](https://arxiv.org/abs/2303.08774)
- **LLaMA**: [Open and Efficient Foundation Language Models (2023)](https://arxiv.org/abs/2302.13971)
- **RAG**: [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (2020)](https://arxiv.org/abs/2005.11401)

📌  **이 단계에서 할 일**

- LLM 모델을 사용해 간단한 프로젝트 진행
- 최신 논문의 구현 코드 찾아보기

---


> NLP 공부 참고 블로그
https://jonhyuk0922.tistory.com/169
https://asidefine.tistory.com/180


> 추천드리는 위키독스, 깃북 페이지
- https://wikidocs.net/book/2788 (딥러닝)  
- https://wikidocs.net/book/2155 (자연어처리)  
- https://kh-kim.github.io/nlp_with_deep_learning_blog/ (자연어처리)  
- https://wikidocs.net/book/8056 (Transformers 라이브러리, 맨 마지막에 공부할 것)  
- https://tutorials.pytorch.kr/beginner/text_sentiment_ngrams_tutorial.html (파이토치 공식 도큐먼트)

- [자연어 처리](https://wikidocs.net/21693)

---
d
