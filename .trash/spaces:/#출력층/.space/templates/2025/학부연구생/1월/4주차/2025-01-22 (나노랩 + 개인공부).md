### 발표준비
Slide-5 (다른 프레임워크 vs Pytorch를 사용하는 이유 )
- **동적 계산 그래프**: 실행 시점에 그래프를 생성하고 수정 가능, 직관적인 디버깅 및 유연한 모델 설계에 유리.
    
- **Python 친화적**: Pythonic한 문법과 인터페이스 제공, 빠르고 간편한 프로토타이핑 가능.
    
- **커뮤니티 및 생태계**: 풍부한 튜토리얼, 강력한 커뮤니티 지원, 다양한 오픈소스 라이브러리와 통합.
    
- **TorchScript**: 동적 그래프의 장점 유지하면서 모델의 배포 및 최적화를 지원.
    
- **GPU 지원**: CUDA를 통해 GPU 가속 학습 및 추론을 간편하게 구현.

---
# 📌 Nano-Lab-Meeting

> 1차원적으로 DB_v1을 쓴것이 문제.

제일 왼쪽에 있는 그래프
-> **시계열그래프로**로 그리기 어떤 방식으로 표현해야 모두 보일지에 대한 고민 
(없애기)

제일 오른쪽에 있는 표를 x 
위/경도를 지도에 찍어서 보여주기 (줌인, 줌아웃)
-> devicde overview 

- 추가 사항은 미리 수정하고 그다음에 보기 
- 데이터를 어떻게 표현하는것이 제일 좋을것인가
- 새 종류에 대한 정보(DB)가 없음 

---
## 💻 추가 공부
### 1. 이미지 데이터와 텐서

#### 📌 이미지 데이터란?

- 이미지 데이터는 픽셀 값들의 집합으로 구성 
- 대부분의 경우, 이미지 데이터는 2D 배열 또는 3D 배열로 표현되며, 각 픽셀은 색상을 표현하는 값들을 가진다. 
- 예를 들어, 흑백 이미지는 2D 배열로 저장되며, RGB 컬러 이미지는 3D 배열로 저장

- **흑백 이미지**: 각 픽셀은 0에서 255 사이의 값을 가지며, 2D 배열로 표현
- **컬러 이미지**: 각 픽셀은 3개의 값(R, G, B)을 가지며, 3D 배열로 표현

>예를 들어, 28×28 크기의 흑백 이미지는 28x28의 2D 배열로, 각 값은 0에서 255 사이의 픽셀 강도를 가진다.

#### 📌 텐서(Tensor)

- 텐서는 다차원 배열을 의미하며, 특히 머신러닝에서는 데이터를 표현하는 데 사용. 
- PyTorch에서는 텐서를 `torch.Tensor`로 나타낸다. 이미지 데이터는 텐서 형태로 처리되는 경우가 많다.

- **스칼라**: 0D 텐서, 하나의 값만을 포함.
- **벡터**: 1D 텐서, 1차원 배열.
- **행렬**: 2D 텐서, 2차원 배열.
- **다차원 배열**: 3D 이상 텐서, 예를 들어 이미지 데이터는 3D 텐서로 표현됩니다. (높이, 너비, 채널)

따라서, 이미지 데이터는 보통 `torch.Tensor` 형태로 변환하여 네트워크에 입력된다. 예를 들어, RGB 이미지의 경우 `(높이, 너비, 3)` 형태로 텐서화된다.

#### 📌 이미지 데이터를 텐서로 변환하기

OpenCV를 사용해 이미지를 불러오고, 이를 PyTorch 텐서로 변환하는 방법은 다음과 같습니다:
```python
import cv2
import torch
import numpy as np

# 이미지 불러오기 (OpenCV로)
image = cv2.imread('path_to_image.jpg')

# BGR -> RGB로 변환
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# 이미지 크기 조정 (옵션)
image = cv2.resize(image, (224, 224))

# 이미지를 텐서로 변환
image_tensor = torch.tensor(image).float()

# 텐서의 차원 (Height, Width, Channels)
print(image_tensor.shape)

```

---
### 2. 활성화 함수 (Activation Function)

#### 활성화 함수란?

- 활성화 함수는 신경망에서 뉴런의 출력을 결정하는 함수 
- 신경망에서 각 층은 입력 데이터를 받아 가중치와 바이어스를 적용한 후, 이 값을 활성화 함수에 통과시켜 출력 값을 생성 
- 이 출력 값은 다음 층의 입력 값으로 사용된다. 
- 활성화 함수는 비선형성을 추가하여 모델이 복잡한 함수도 학습할 수 있게 한다.

#### **자주 사용되는 활성화 함수**

- **ReLU (Rectified Linear Unit)**: 음수 입력은 0으로, 양수 입력은 그대로 출력하는 함수입니다. 학습 속도가 빠르고, 음수 부분에서 기울기가 0이 되어 신경망 학습에서 발생할 수 있는 문제인 "기울기 소실"을 완화시킨다.
    $$
    ReLU(x)= \max(0, x)\
    $$
- **Sigmoid**: 출력 값이 0과 1 사이로 압축된다. 주로 이진 분류 문제에서 사용됨.
    $$
    σ(x) =  \frac{1}{1 + e^{-x}}
    $$​
- **Tanh**: 출력 값이 -1과 1 사이로 압축된다. Sigmoid와 비슷하지만, 출력 범위가 더 넓어 일반적으로 더 빠른 학습을 제공한다.
    $$
    tanh(x)== \frac{2}{1 + e^{-2x}} - 1
    $$
- **Softmax**: 다중 클래스 분류에서 주로 사용됩니다. 출력값을 확률처럼 변환하여, 각 클래스에 대한 확률을 제공

```python
# 활성화 함수 예시 
import torch
import torch.nn as nn

# ReLU 활성화 함수 사용 예시
relu = nn.ReLU()
x = torch.tensor([-1.0, 2.0, -0.5, 3.0])
output = relu(x)
print(output)  # tensor([0., 2., 0., 3.])

```


---
### 3. 기울기 조정 (Gradient Descent)

- 신경망은 입력 데이터를 통해 예측을 하고, 그 예측값과 실제 값 사이의 오차를 계산한 뒤, 그 오차를 최소화하기 위해 가중치와 바이어스를 업데이트한다. 
- 이 과정에서 기울기(gradient)를 계산하고, 이를 바탕으로 가중치를 조정

#### 📌 기울기 하강법 (Gradient Descent)

기울기 하강법은 함수의 기울기를 계산하여 최솟값을 찾는 최적화 방법입니다. 기울기는 함수의 변화율을 나타내며, 기울기가 0인 지점에서 함수는 최솟값을 가질 수 있다.

**기울기 하강법**에서 사용되는 기본 공식은 다음과 같다:

$$
w=w−η⋅∇L(w)
$$

- w: 가중치
- η: 학습률(learning rate)
- ∇L(w) : 손실 함수의 기울기

#### 기울기 조정 예시 (PyTorch)

- PyTorch에서는 자동으로 기울기를 계산하고 가중치를 업데이트하는 기능이 내장되어 있다. 
  - 예를 들어, 다음과 같이 신경망을 정의하고, 기울기를 계산하고, 가중치를 업데이트할 수 있다.

```python
## 기울기 조정 예시

import torch
import torch.nn as nn
import torch.optim as optim

# 간단한 신경망 모델 정의
model = nn.Sequential(
    nn.Linear(2, 2),
    nn.ReLU(),
    nn.Linear(2, 1)
)

# 손실 함수와 최적화 방법 설정
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 데이터
inputs = torch.tensor([[1.0, 2.0]])
target = torch.tensor([[0.0]])

# 기울기 초기화
optimizer.zero_grad()

# 모델 출력
output = model(inputs)

# 손실 계산
loss = criterion(output, target)

# 기울기 계산
loss.backward()

# 가중치 업데이트
optimizer.step()

print("손실:", loss.item())

```

---
## 💻 실습 예시(OpenVc와 PyTorch로 이미지 처리 및 기울기 조정)
```python
import cv2
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

image = cv2.imread('image.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image = cv2.resize(image, (224, 224))

# 2. 이미지 -> 텐서 변환
image_tensor = torch.tensor(image).float().permute(2, 0, 1).unsqueeze(0)  # (1, C, H, W)

# 3. 간단한 신경망 모델 정의
model = nn.Sequential(
    nn.Conv2d(3, 16, 3),
    nn.ReLU(),
    nn.Flatten(),
    nn.Linear(16 * 222 * 222, 10)
)

# 4. 손실 함수와 최적화 방법 
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

target = torch.tensor([1])

# 6. 기울기 계산 및 업데이트
optimizer.zero_grad()
output = model(image_tensor)
loss = criterion(output, target)
loss.backward()
optimizer.step()

print(f'손실: {loss.item()}')

```

> [파이썬 배우기](https://wikidocs.net/60034)
> 