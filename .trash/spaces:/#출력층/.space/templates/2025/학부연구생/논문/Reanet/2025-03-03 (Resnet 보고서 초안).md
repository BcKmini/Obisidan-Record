Deep Residual Learning for Image Recognition - Review

학과 : 소프트웨어학과

학번 : 20202020827

학년 : 4학년

이름 : 김경민

---

## **1.** **서론**

·  **연구 동기(왜 이 논문이 쓰였는가?)**

- 딥러닝 분야에서 네트워크의 **깊이(depth)**를 늘리는 것은 모델의 **표현력**을 향상시키는 대표적인 방법이다. 그러나 층이 깊어질수록 **기울기 소실(vanishing gradient)**, **학습 불안정성**, 그리고 네트워크가 깊어짐에도 성능이 오히려 떨어지는 **Degradation 문제**가 발생한다.
- 이러한 문제를 해결하기 위해 He et al.는 **Residual Learning(잔차 학습)**을 제안하였고, 이를 통해 매우 깊은 신경망에서도 효율적이고 안정적으로 학습할 수 있는 **ResNet(Residual Network)** 구조를 고안하였다.

·  **논문이 다루는 핵심 목표**

1. 네트워크가 깊어질수록 발생하는 **Degradation 문제**를 해소할 수 있는 새로운 구조(Residual Block) 제안
2. **기존의 단순(plain) 네트워크**와 **잔차 연결(ResNet)**을 적용한 네트워크를 비교하여, 깊이가 깊어질 때도 학습 오류가 증가하지 않고 오히려 성능이 향상되는 현상을 실험적으로 입증
3. ImageNet, CIFAR-10 등 다양한 데이터셋에서 모델을 학습·평가함으로써, ResNet이 기존 모델 대비 더 우수한 결과를 낸다는 점을 검증

---

## 2.  **Degradation** **문제: 20-layer vs. 56-layer Plain Network**
![[Pasted image 20250303123023.png]]

- 논문에서는 “Degradation 문제”가 네트워크 깊이를 단순히 늘렸을 때 발생한다고 지적함.
- 특정 깊이를 넘어서면, 오히려 더 깊은 네트워크가 **학습 난이도**와 **기울기 소실** 문제로 성능이 떨어지는 현상이 발생.
- 위 그림은 **56-layer** plain 네트워크가 20-layer보다 높은 오류율을 보이는 모습을 시각적으로 보여줌.

- 깊이가 늘어나면 모델 표현력은 증가하지만, **기울기 소실(vanishing gradient)** 문제로 학습이 제대로 이루어지지 않을 수 있음.
- “깊으면 무조건 좋다”는 단순 가정이 틀렸음을 보여주는 사례로, 네트워크 구조 자체에 개선이 필요함

---
## 3. **Residual Block** **구조:** **y**

![[Pasted image 20250303123028.png]]


입력 x가 여러 합성곱·활성화 함수를 거친 결과 F와 **직접 더해**(skip connection) 다음 단계로 전달.

- 논문은 잔차 연결(Residual Connection)을 통해 깊은 네트워크에서도 기울기 소실 문제를 완화할 수 있다고 주장.
- F(x) 는 합성곱·배치 정규화·ReLU 등 일반적인 연산으로 구성되지만, 마지막에 x를 그대로 더해주므로 **정보 손실**이 줄어들고 **역전파**가 원활해짐.

**(2)** **해석**

- 기존의 “Plain” 구조는 입력이 층을 지날 때마다 변환만 쌓이지만, **ResNet**은 입력을 그대로 전달.
- 이를 통해 네트워크가 깊어져도 **초기 정보**가 소실되지 않고, **기울기**도 더 안정적으로 역전파될 수 있음.
- 사실상 “깊은 네트워크” 학습에서 가장 큰 문제였던 기울기 소실을 해결가능

#### **📌** **관련 실험 (FC20 vs. FC56 vs ResNet-18 )**

![[Pasted image 20250303123035.png]]

|   |   |   |   |   |
|---|---|---|---|---|
|**Mode**|**Train Loss**|**Val Loss**|**Train Accuracy**|**Val Accurcay**|
|**FC20**|**2.2377**|**2.1482**|**0.1419**|**0.1727**|
|**FC56**|**2.307**|**2.3029**|**0.0999**|**0.0965**|
|**ResNet**|**0.0043**|**0.9452**|**0.9545**|**0.7982**|

-      **CIFAR-10**에서 **ResNet-18** 모델을 사용해 본 결과:

- FC20/FC56 대비 훨씬 빠른 손실 감소와 안정적 학습 곡선
- 훈련 정확도 **90% 이상**, 검증 정확도 약 79%로, 단순 FC 모델에 비해 월등히 높은 성능

- 이는 논문에서 말한 “잔차 연결로 인해 깊어져도 성능이 떨어지지 않는다”는 점을 실제 코드로 재현
- **CIFAR-10** 데이터셋에서 **FC20**(20층 완전 연결)과 **FC56**(56층 완전 연결)을 비교.

- FC20: 약 **16%** 내외의 낮은 정확도
- FC56: 깊이가 더 깊음에도 불구하고 정확도가 **10% 이하**로, 오히려 더 낮아짐

- 이는 논문에서 말한 Degradation 문제와 동일한 현상. 단순히 층만 늘리면 성능이 올라가지 않고, 최적화 어려움으로 학습이 불안정해지는 결과를 확인.

---

## **4. VGG-19 vs. 34-layer Plain vs. 34-layer Residual**

![[Pasted image 20250303123042.png]]

**(****왼쪽) - VGG-19**: 모든 합성곱을 3×3 필터로 통일해 층을 깊게 구성

**(****가운데) - 34-layer plain**: 34층까지 단순히 쌓은 구조 (잔차 연결 없음)

**(****오른쪽) - 34-layer residual**: 동일한 34층이지만, 잔차 연결을 도입한 ResNet

- 논문에서는 **VGG**와 **Plain** 네트워크, 그리고 **Residual** 네트워크를 구조적으로 비교함.
- 결과적으로 **Plain** 모델은 깊이가 깊어질수록 Degradation 문제로 성능이 낮아지고, **Residual** 모델은 오히려 깊어질수록 성능이 개선됨을 강조.

**(2)** **해석**

- VGG 역시 일정 수준 이상의 성능을 보이지만, **3×3** **필터 반복**과 **잔차 연결**은 별개의 개념.
- VGG는 깊이가 깊어지면 여전히 기울기 소실 문제를 겪을 수 있으나, ResNet의 잔차 연결은 이 문제를 해결해 **더 깊은 층**에서도 안정적인 학습을 가능케 함.

#### **📌** **관련 실험 (나의 PlainCNN vs. ResNet vs. VGG, CIFAR-100)**

- **CIFAR-100**에서 **PlainCNN(18, 34)**, **ResNet(18, 34)**, VGG(16, 19)를 비교 실험.

- PlainCNN 모델들은 클래스가 100개로 늘어나자 **에러율이 높고 학습이 불안정**.
- VGG16/19는 나쁘지 않은 성능이나, **ResNet18/34**가 에러율 면에서 더 우수한 결과를 보임.

- 이는 논문에서 지적한 대로, **깊은 네트워크에서도 잔차 연결**이 핵심적인 역할을 한다는 사실을 재확인해 준다.

---
## **5. ImageNet** **성능 비교**

![[Pasted image 20250303123050.png]]

![[Pasted image 20250303123059.png]]

·       **Plain-18 vs. ResNet-18**: Top-1 오류율이 27.94% vs. 27.88% (큰 차이는 아니지만 ResNet이 근소하게 우세)

·       **Plain-34 vs. ResNet-34**: 28.54% vs. 25.03%로, **ResNet-34****가 확실히 우수**

·       파라미터 수가 크게 늘지 않았음에도 불구하고, 잔차 연결로 인해 성능이 향상됨을 시사

- ImageNet처럼 대규모 데이터셋에서도 **Residual Learning**이 효과적임을 수치로 입증.
- 깊이가 34층이 될 때, **Plain** 네트워크는 오류율이 급격히 올라가지만, **ResNet**은 25%대의 오류율로 상대적으로 낮은 편.

**(2)** **해석**

- 논문 전반의 메시지를 요약해주는 표로, “깊이가 같아도 **구조적 차이**(잔차 연결)로 인해 성능이 달라진다”는 점을 수치로 확인 가능.
- 특히 34층에서의 3% 이상 차이는 매우 의미가 크며, 이후 50층·101층·152층까지 확장되면서 ResNet의 우수성이 더 명확히 드러남.

#### **📌** **관련 실험 (내 실험과의 간단 비교)**

- ImageNet만큼 큰 스케일은 아니지만, **CIFAR-10/100**에서도 Plain vs. ResNet을 비교했을 때,

- Plain 쪽은 학습 난이도로 인해 오류율이 높게 유지
- ResNet 쪽은 빠른 수렴과 낮은 에러율 기록

- 비록 데이터셋 규모는 다르지만, 논문에서 보여준 경향이 **동일하게 재현**되었다고 볼 수 있음.

---
## 6. **CIFAR-10** **추가 결과: FitNet, Highway Network, ResNet 등 비교**
![[Pasted image 20250303123108.png]]

**(CIFAR-10** **분류 에러)**
- FitNet, Highway Network, Maxout, DSN 등 기존 모델과 ResNet을 비교
- ResNet-110, ResNet-1202 등이 **오류율 6~7%대**로 다른 모델 대비 성능 우위


- 논문은 CIFAR-10에서도 다양한 기존 모델(Highway Network, FitNet, DSN 등)과 비교 실험을 진행.
- **ResNet-110**: 약 **6.43~6.61%**의 오류율로 기존 모델보다 낮은 에러율을 달성.
- 극단적으로 **ResNet-1202**층까지 쌓아도, 여전히 오류율이 7%대 수준으로 준수한 성능을 보여줌.

### (2) 해석

- ResNet은 단순히 ImageNet뿐 아니라, **소규모/중규모 데이터셋(CIFAR-10/100)**에서도 좋은 성능을 낸다는 점을 강조.
- Highway Network처럼 **직접 연결**을 활용한 모델들도 있지만, ResNet의 skip connection 방식이 구현이 단순하면서 효과가 크다고 볼 수 있음.

---

## **결론 및 요약**

1. **Degradation** **문제 해결**

- 단순히 네트워크 깊이를 늘리는 Plain 모델은 기울기 소실로 인해 오히려 성능이 저하될 수 있음(FC20 vs. FC56 실험, 20-layer vs. 56-layer 그래프 등).

1. **Residual Block**의 역할

- y=F(x)+x 구조로 입력을 직접 더해주는 방식이 **깊은 층**에서도 기울기 전파를 돕고, 학습을 안정화시킴(ResNet-18/34/50, 등등).

1. **다양한 데이터셋에서 우수성 검증**

- 논문은 ImageNet 대규모 데이터셋과 CIFAR-10/100에서 **ResNet**의 성능 우수성을 입증.
- 실제로 내가 진행한 FC vs. ResNet, PlainCNN vs. ResNet/VGG 실험에서도 **잔차 연결**이 있을 때 훨씬 나은 결과를 얻음.

1. **의의**

- 딥러닝 모델 설계에서 “층을 깊게 쌓는 것”만으로는 한계가 있음을 재확인.
- **Residual Learning**이 이후 다양한 변형 모델(Pre-activation ResNet, ResNeXt 등)과 산업 응용으로 이어졌다는 점에서, 본 논문은 매우 큰 의미를 지님.
