### **눈문 :  Deep Residual Learning for Image Recognition**

#### **1. 실험 배경 및 목적**

딥러닝 모델의 성능을 향상시키기 위해 네트워크의 **층(depth)**을 증가시키는 접근이 널리 사용되었습니다. 그러나 네트워크의 깊이가 깊어지면 **기울기 소실(vanishing gradient)** 문제나 **학습 불안정성** 등이 발생할 수 있다는 우려가 제기되어 왔습니다. 이러한 문제를 해결하기 위해 **ResNet(Residual Networks)** 모델은 **잔차 학습(residual learning)** 개념을 도입하여, 깊은 네트워크에서도 효과적인 학습을 가능하게 했습니다.
본 테스트는 논문에 나온 **네트워크 깊이**가 성능 향상에 미치는 영향을 평가하고자 하였으며, **ResNet-18** 모델을 사용하여 **잔차 연결**이 네트워크 깊이를 깊게 하는 것만으로 성능을 높일 수 있는지 여부를 실험적으로 확인하고자 했습니다. 기존의 **FC20**과 **FC56** 모델을 통해 네트워크의 깊이를 늘리는 것이 실제로 성능 향상에 기여하는지 확인하고, **ResNet-18** 모델을 통해 잔차 연결이 이를 해결하는지 확인했습니다.

#### **2. 실험 방법**

- **모델 설계**:
    - **ResNet-18**: 잔차 연결(residual connection)을 포함한 네트워크로, 깊은 네트워크에서도 기울기 소실 문제를 해결하여 안정적인 학습을 가능하게 합니다. CIFAR-10 데이터셋에 맞게 출력층을 수정한 ResNet-18 모델을 사용하였습니다.
    - **FC20**: 20개의 완전 연결층으로 구성된 모델로, 각 층은 4096개의 뉴런을 가지고 있으며 ReLU 활성화 함수와 Dropout을 사용하여 과적합을 방지하려 했습니다.
    - **FC56**: FC20보다 더 깊은 56개의 완전 연결층으로 설계된 모델입니다. 깊이를 증가시키는 것이 성능 향상에 영향을 미칠지 실험하기 위한 모델입니다.
    
- **데이터셋**:
    - 논문에 내용과 같은 **CIFAR-10** 데이터셋을 사용
    - 이 데이터셋은 10개의 클래스를 포함하며, 총 60,000개의 32x32 픽셀 크기의 이미지로 구성되어 있습니다.
    
- **훈련**:
    - 훈련은 **Adam Optimizer**와 **Cross Entropy Loss**를 사용
    - 각 모델은 30 에폭(epoch)
    - **배치 크기**는 32
    - **학습률**은 0.001로 설정

#### **3. 실험 결과**
**ResNet-18** 모델은 **잔차 학습**을 통해 깊은 네트워크에서도 안정적인 학습을 가능하게 했습니다. **FC20** 및 **FC56** 모델의 성능과 비교한 결과는 다음과 같습니다:
![[Pasted image 20250228200337.png]]

- **ResNet-18**:
    - 훈련 손실과 검증 손실은 급격히 감소하였으며, 훈련 정확도는 80% 이상, 검증 정확도는 79% 이상을 기록하였습니다.
    - Precision과 Recall 모두 60% 이상의 성능을 보였으며, **잔차 학습**이 효과적으로 기울기 소실 문제를 해결한 것을 확인할 수 있었습니다.
    
- **FC20**:
    - 훈련 정확도는 16% 내외로 매우 낮았고, 검증 정확도도 16%로 낮았습니다. 손실 값도 상대적으로 높았으며, Precision과 Recall도 낮은 값을 기록했습니다.
    
- **FC56**:
    - FC20보다 훈련 정확도가 더 낮았고, 검증 정확도 역시 매우 낮았습니다. Precision과 Recall은 10% 이하로, 모델의 성능이 매우 저조했습니다.

## 📌  테이블 표

| Model | Train Loss | Val Loss | Train Accuracy | Val Accuracy | Precision | Recall |
|-------|------------|----------|----------------|--------------|-----------|--------|
| FC20  | [2.2196, 2.2181, 2.224... ] | [2.1830, 2.1274, 2.1638... ] | [0.1651, 0.1563, 0.1617, 0.1536, 0.1598, ... ] | [0.1794, 0.1818, 0.1802, 0.1926, 0.1614, ... ] | [0.2640, 0.2478, 0.2504... ] | [0.1648, 0.1562, 0.1627, 0.1611, ... ] |
| FC56  | [2.3323, 2.3043, 2.3103... ] | [2.3027, 2.3029, 2.3028... ] | [0.0991, 0.0965, 0.0983, 0.0989, 0.1002, ... ] | [0.0948, 0.0968, 0.0948, 0.0968, 0.0968, ... ] | [0.0384, 0.0857, 0.1002... ] | [0.0981, 0.0960, 0.0904, 0.0965, ... ] |
| ResNet | [1.1383, 0.8080, 0.660... ] | [0.8600, 0.6965, 0.618... ] | [0.6191, 0.7321, 0.7820, 0.8198, 0.8470, ... ] | [0.7036, 0.7630, 0.7910, 0.7928, 0.7868, ... ] | [0.6168, 0.7308, 0.7811... ] | [0.6188, 0.7320, 0.7819, ... ] |



#### **4. 결과 분석**
본 실험의 핵심은 **레이어의 깊이가 성능에 미치는 영향**을 평가하는 것이었습니다. **FC20**과 **FC56** 모델을 통해 네트워크 깊이가 성능 향상에 어떻게 영향을 미치는지 분석했습니다.

- **FC20**과 **FC56** 모델은 **단순한 완전 연결 네트워크**로 구성되어 있지만, 네트워크의 깊이가 깊어짐에 따라 성능이 저조해졌습니다. 이는 과적합의 가능성이나 학습의 어려움, **기울기 소실 문제** 등이 발생했기 때문으로 해석됩니다. 모델의 깊이가 늘어날수록 성능이 개선되지 않고 오히려 **학습이 불안정**해졌습니다.
    
- 반면, **ResNet-18** 모델은 **잔차 연결**을 통해 깊은 네트워크에서 발생할 수 있는 기울기 소실 문제를 해결하였고, 안정적인 학습을 가능하게 했습니다. **잔차 학습**은 네트워크의 깊이를 깊게 해도 성능 저하 없이, 오히려 성능 향상으로 이어졌습니다.
    

#### **5. 결론**
본 실험을 통해 **네트워크 깊이**가 성능 향상에 미치는 영향을 확인
**FC20**과 **FC56** 모델은 네트워크의 깊이를 증가시켰음에도 불구하고 성능이 개선되지 않았다. 이는 네트워크가 너무 깊어져 **기울기 소실** 문제나 **과적합** 등의 문제가 발생했기 때문이다.
반면, **ResNet-18** 모델은 **잔차 학습**을 통해 깊은 네트워크에서도 효과적인 학습을 가능하게 했으며, 훈련 및 검증 정확도, Precision, Recall 모두에서 우수한 성능을 보였다. 이는 **잔차 학습**이 깊은 네트워크에서 성능을 향상시키는 데 중요한 역할을 한다는 것을 보여준다.
따라서 **딥러닝 모델의 깊이**를 늘리기만 하는 접근은 성능 향상에 한계가 있으며, **잔차 연결(residual connections)** 에 관한 논문에 내용을 더 읽어보려고 한다.

---


