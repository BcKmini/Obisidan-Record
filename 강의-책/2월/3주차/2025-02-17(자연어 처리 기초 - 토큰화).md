# 토큰화(Tokenization): 자연어 처리의 첫걸음 🚀

자연어 처리(NLP)에서 수집한 텍스트 데이터는 우리가 바로 사용할 수 있는 형태가 아닙니다. 크롤링 등을 통해 얻은 코퍼스(corpus)는 보통 여러 가지 불필요한 요소를 포함하고 있으며, 분석하고자 하는 목적에 맞게 전처리 과정이 필요합니다. 이 전처리 과정에는 크게 **토큰화(tokenization), 정제(cleaning), 정규화(normalization)**가 포함됩니다.

이번 포스트에서는 그중에서도 **토큰화(Tokenization)**에 대해 집중적으로 다루겠습니다. 또한 **NLTK, KoNLPY 등의 라이브러리를 활용하여 토큰화를 실습**해보겠습니다.

---

## 📌 1. 토큰화(Tokenization)란?

토큰화(Tokenization)란 **텍스트를 일정한 단위로 나누는 작업**을 의미합니다. 여기서 나누어진 단위를 **토큰(token)**이라고 하며, 일반적으로 의미 있는 단위로 나누는 것이 중요합니다.

토큰의 기준은 상황에 따라 다를 수 있습니다.

- 단어(word) 단위로 나누면 **단어 토큰화(Word Tokenization)**  
- 문장(sentence) 단위로 나누면 **문장 토큰화(Sentence Tokenization)**

이제 단어 토큰화부터 살펴보겠습니다.

---

## 📌  2. 단어 토큰화(Word Tokenization)

단어 토큰화는 텍스트를 **단어 기준으로 분리하는 과정**입니다. 하지만 단순히 공백을 기준으로 나누는 것만으로는 해결되지 않는 경우가 많습니다.

예제를 통해 살펴보겠습니다.
### 단어 토큰화 예제
```python
from nltk.tokenize import word_tokenize

text = "Time is an illusion. Lunchtime double so!"
print(word_tokenize(text))

# 출력결과
['Time', 'is', 'an', 'illusion', '.', 'Lunchtime', 'double', 'so', '!']
```

이처럼 단어 토큰화 과정에서 **구두점(punctuation)도 별도의 토큰으로 인식**됩니다.
### 단순 공백 분리는 부족하다!

만약 띄어쓰기 기준으로만 나누면 `"illusion."`과 같은 단어에서 마침표가 포함된 상태로 출력될 수 있습니다. 즉, 단순히 공백 기준으로 나누면 제대로 된 토큰화가 어렵다는 것이죠.
더 나아가, **아포스트로피(')가 포함된 단어**는 어떻게 처리해야 할까요?

---

## 🤔 3. 토큰화 시 고려해야 할 문제들

토큰화 과정에서 여러 선택의 순간이 존재합니다. 그중 대표적인 문제를 살펴보겠습니다.

### (1) 아포스트로피(') 처리는 어떻게?
다음 문장을 보겠습니다.
```text
Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.
```
여기서 **"Don't"**, **"Jone's"**는 어떻게 나누어야 할까요?
NLTK의 `word_tokenize`와 `WordPunctTokenizer`를 비교해 보겠습니다.
```python
from nltk.tokenize import word_tokenize, WordPunctTokenizer

text = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."

print("word_tokenize:", word_tokenize(text))
print("WordPunctTokenizer:", WordPunctTokenizer().tokenize(text))

# 출력결과
word_tokenize: ['Do', "n't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', "'s", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']

WordPunctTokenizer: ['Don', "'", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', "'", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']
```
- **`word_tokenize`**는 `"Don't"`를 `"Do"`와 `"n't"`로 분리  
- **`WordPunctTokenizer`**는 `"Don't"`를 `"Don"`, `"'"`, `"t"`로 더 세밀하게 분리

이처럼 **어떤 토큰화 방식을 선택하느냐에 따라 결과가 달라질 수 있습니다**.

---
## 📌 4. 문장 토큰화(Sentence Tokenization)

이번에는 텍스트를 **문장 단위로 나누는** 문장 토큰화를 살펴보겠습니다.
### 문장 토큰화 예제
```python
from nltk.tokenize import sent_tokenize

text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy."
print(sent_tokenize(text))

# 출력결과
['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.']
```
-  `sent_tokenize()`는 문장을 성공적으로 나눕니다.  
-  하지만 **마침표(.)가 모든 경우 문장의 끝을 의미하지는 않습니다**.
다음과 같은 경우는 어떨까요?
```python
text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
print(sent_tokenize(text))

# 출력결과
['I am actively looking for Ph.D. students.', 'and you are a Ph.D. student.']
```
- **"Ph.D."**의 마침표는 문장의 끝이 아니라는 것을 잘 인식하여 문장을 제대로 분리하였습니다.
---
## 🇰🇷 5. 한국어에서의 토큰화 문제 

영어는 띄어쓰기가 비교적 규칙적이기 때문에 **공백 기준의 토큰화가 잘 동작**합니다.  
하지만 **한국어는 교착어**이기 때문에 단순한 띄어쓰기 기준의 토큰화가 어렵습니다.
### (1) 조사(은/는/이/가) 문제
예를 들어 **"그가", "그를", "그에게"**는 모두 **"그"**라는 단어에 조사(가, 를, 에게)가 붙은 형태입니다.  
만약 띄어쓰기 기준으로만 토큰화한다면, **같은 단어임에도 다르게 인식될 수 있습니다**.

### (2) 띄어쓰기 문제
한국어는 띄어쓰기가 영어보다 덜 엄격하게 지켜지는 경우가 많습니다.
```text
제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.
```
위 문장을 띄어쓰기 기준으로 나눈다면 제대로 된 결과를 얻기 어렵습니다.

---

 ## 📌 6. KoNLPy를 활용한 한국어 토큰화 및 품사 태깅

한국어는 **교착어의 특성을 가지기 때문에** 띄어쓰기 단위로 토큰화하는 것이 어렵습니다.  
이를 해결하기 위해 **KoNLPy** 패키지의 형태소 분석기를 활용하여 **형태소 단위 토큰화**를 수행합니다.

KoNLPy에서 사용할 수 있는 형태소 분석기:

- **Okt(Open Korea Text)**
- **Mecab**
- **Komoran**
- **Hannanum**
- **Kkma**
이 중에서 **Okt와 Kkma** 두 가지 형태소 분석기를 사용하여 비교해보겠습니다.
#### KoNLPy의 Okt 형태소 분석기 실습
```python
from konlpy.tag import Okt

okt = Okt()
text = "열심히 코딩한 당신, 연휴에는 여행을 가봐요."

print('OKT 형태소 분석 :', okt.morphs(text))
print('OKT 품사 태깅 :', okt.pos(text))
print('OKT 명사 추출 :', okt.nouns(text))

# 출력결과 
OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']
OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]
OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']

```

>🔹 **Okt 형태소 분석기 특징**

1. `morphs()` → 문장을 형태소 단위로 나눔
2. `pos()` → 형태소에 품사 태깅
3. `nouns()` → 명사만 추출
#### KoNLPy의 Kkma 형태소 분석기 실습
```python
from konlpy.tag import Kkma

kkma = Kkma()
text = "열심히 코딩한 당신, 연휴에는 여행을 가봐요."

print('꼬꼬마 형태소 분석 :', kkma.morphs(text))
print('꼬꼬마 품사 태깅 :', kkma.pos(text))
print('꼬꼬마 명사 추출 :', kkma.nouns(text))

# 출력결과
꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']
꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]
꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']
```

> 각 형태소 분석기는 성능과 결과가 다르게 나오기 떄문에, 형태소 분석기의 선택은 사용하고자 하는 필요 용도에 어떤 형태소 분석기사 가장 적절한지를 판단하고 사용하면 된다. 