·  **모델 구성**

- PlainCNN18/34: 단순 CNN 구조, convolution 레이어와 FC 레이어를 통해 100개 클래스로 출력
- ResNet/VGG: 사전 모델로, 마지막 출력층을 CIFAR-100에 맞게 수정

·  **데이터 전처리 및 구성**

- 이미지 리사이즈: 224×224
- 텐서 변환 및 ImageNet 통계(mean, std)로 정규화
- CIFAR-100 데이터셋을 80:10:10 비율로 학습/검증/테스트 분할

·  **학습 설정**

- 손실 함수: CrossEntropyLoss
- 옵티마이저: SGD (lr=0.001, momentum=0.9)
- 총 에폭 수: 40
- 에폭별로 손실, 정확도, 에러율 출력 및 기록

### 아래는 테스트 성능을 중심으로 요약한 표

| 모델         | 검증 정확도 | 테스트 정확도 | 테스트 에러율 |
| ---------- | ------ | ------- | ------- |
| PlainCNN18 | 31.24% | 32.04%  | 68.00%  |
| PlainCNN34 | 33.64% | 34.72%  | 65.28%  |
| ResNet18   | 78.52% | 78.40%  | 21.60%  |
| ResNet34   | 79.82% | 80.02%  | 20.00%  |
| VGG16      | 76.34% | 75.56%  | 24.44%  |
| VGG19      | 76.32% | 75.70%  | 24.30%  |

---


![[3번.png]]

## 1. 공통 배경: ImageNet 분류를 위한 전형적인 세팅

- **입력 이미지**: 일반적으로 224×224 해상도의 3채널(RGB) 이미지가 입력
- **출력**: 1000개의 클래스를 예측하는 분류 결과
- **공통 Downsampling 방식**:
    - 첫 번째 레이어에서 보통 stride=2인 7×7 컨볼루션(혹은 3×3 Max Pool)을 통해 해상도가 112×112로 줄어듦
    - 이후에도 블록이 바뀔 때마다 stride=2 등의 설정으로 공간 해상도를 절반으로 줄이면서 채널 수는 2배로 증가

---

## 2. VGG-19 (왼쪽 구조)

1. **구조 개요**
    
    - 19개의 학습 가능한 레이어(컨볼루션/FC 등)로 구성
    - 3×3 컨볼루션을 여러 번 연속으로 쌓고, 그 사이마다 Max Pooling 레이어로 공간 해상도를 줄임
    - 마지막에 FC 레이어(주로 4096 차원 2개 + 1000 클래스 분류 레이어)로 연결
2. **채널(필터) 구성 예시**
    
    - 입력(224×224) → (Conv 3×3 × N번) → Pool(112×112)
    - (Conv 3×3 × N번) → Pool(56×56)
    - (Conv 3×3 × N번) → Pool(28×28)
    - (Conv 3×3 × N번) → Pool(14×14)
    - (Conv 3×3 × N번) → Pool(7×7)
    - 이후 FC 레이어(7×7의 feature map을 1차원으로 펼친 뒤 분류)
3. **FLOPs(연산량) 큰 이유**
    
    - VGG 계열은 레이어별 필터 수가 상대적으로 많음(예: 64 → 128 → 256 → 512 → 512 ...)
    - 3×3 필터를 여러 번 중첩해서 쓰다 보니, 전체 연산량이 약 19.6억(=19.6 GFLOPs) 정도로 큼
        - 여기서 논문 그림에선 19.6 GFLOPs라고 했지만, 여러 구현 버전에 따라 약간씩 다를 수 있음

---

## 3. 34-layer Plain (가운데 구조)

1. **구조 개요**
    
    - ResNet 논문에서 제안한 34층짜리 네트워크의 ‘Residual Connection(스킵 연결)’을 제거한 버전
    - 즉, 단순히 3×3 컨볼루션 레이어를 많이(34층에 맞게) 쌓은 형태
    - Downsampling은 VGG처럼 Pooling 대신, 특정 레이어에서 stride=2인 컨볼루션을 사용해 진행하기도 함
2. **흐름(계층 순서)**
    
    - **7×7 Conv(stride=2)** → 출력 맵 크기: 112×112
    - **3×3 Max Pool(stride=2)** → 56×56
    - **3×3 Conv 레이어 여러 개**
        - 채널: 64 → 128 → 256 → 512로 바뀌며, 공간 해상도는 56×56 → 28×28 → 14×14 → 7×7로 절반씩 다운샘플
    - **Global Average Pooling**(7×7을 1×1로 만듦)
    - **FC 레이어(1000 클래스)**
3. **채널과 레이어 수**
    
    - 예시로 (3,4,6,3) 블록처럼 묶어서 각 블록마다 3×3 Conv를 2개씩 쌓는 식으로 구성
    - 초기 7×7 Conv + (3+4+6+3) 블록 × 2개의 3×3 Conv = 총 33개, 마지막 FC 포함하면 34개 레이어로 계산
    - Plain 구조에서는 skip 연결이 없고, 단순히 컨볼루션만 이어짐
4. **FLOPs가 3.6억(=3.6 GFLOPs) 정도인 이유**
    
    - VGG-19 대비 **필터 수가 조금 더 효율적으로 배분**되어 있고, **특정 레이어에서만 해상도를 크게 줄이는(7×7 Conv, stride=2 등) 방식**을 택함
    - 따라서 레이어가 34개로 더 깊지만, 연산량은 3.6 GFLOPs로 VGG-19보다 훨씬 작음

---

## 4. 34-layer Residual (오른쪽 구조)

1. **Residual Block(잔차 블록)의 개념**
    
    - Plain 구조(가운데)와 동일한 3×3 Conv 개수를 갖지만, **블록 단위로 skip 연결(Identity 혹은 1×1 Conv projection)을 추가**
    - ‘잔차(residual)’를 더해주는 연결 구조로, 역전파 시 그래디언트 소실 문제를 완화
2. **구조 흐름**
    
    - 7×7 Conv(stride=2) + Max Pool → (3×3 Conv × 2 + skip) × (3+4+6+3) 블록 → Global Average Pooling → FC(1000)
    - **스킵 연결**은 보통 두 개의 3×3 Conv 레이어를 한 묶음으로 하여, 입력 x를 그대로 출력에 더해 y = F(x) + x 형태로 계산
    - 채널이 바뀔 때(예: 64 → 128)에는 ‘dotted shortcut’(점선)처럼 1×1 Conv를 써서 차원 맞춤
3. **FLOPs가 Plain과 동일하게 3.6억**
    
    - Residual 버전이라고 해서 특별히 연산이 크게 늘지 않음(스킵 연결은 단순 덧셈이므로 연산량이 미미)
    - 따라서 Plain 버전과 동일한 3.6 GFLOPs

---

## 5. 그림 속 숫자의 의미

1. **블록별 출력 크기와 채널 수**
    
    - 예: “56×56, 64” → 공간 해상도 56×56, 채널 64
    - 이후 stride=2 컨볼루션(혹은 Pool)로 28×28, 채널 128 → 14×14, 채널 256 → 7×7, 채널 512 순서로 다운샘플
    - VGG-19도 비슷하지만, 각 구간에서 컨볼루션을 여러 번 반복하면서 채널 수가 계속 유지되거나 2배로 증가
2. **전체 레이어 수 계산**
    
    - VGG-19: 컨볼루션 레이어(3×3) 여러 개 + FC 레이어(3개) = 19개
    - 34-layer Plain/Residual: (7×7 Conv + 여러 3×3 Conv 블록 + FC) = 총 34개
    - 블록별로 “3×3 Conv가 몇 개 포함되었는지”를 합산해서 최종적으로 34개가 됨
3. **FLOPs(연산량) 표기**
    
    - 논문에서 제시된 수치:
        - VGG-19: 19.6 GFLOPs(약 20억 번의 부동소수점 연산)
        - 34-layer Plain/Residual: 3.6 GFLOPs
    - 이는 **입력 크기 224×224, 배치 1** 기준으로 단순 계산한 값이며, 구현상 약간의 차이는 있을 수 있음

---

## 6. 정리

- **VGG-19(왼쪽)**
    
    - 3×3 컨볼루션을 여러 번 반복 + Max Pool로 다운샘플
    - 마지막에 FC 레이어 3개로 분류
    - 레이어는 19개지만 필터 수가 많아 FLOPs가 약 19.6G로 큼
- **34-layer Plain(가운데)**
    
    - Residual Connection 없이 3×3 컨볼루션만 34층 쌓은 구조
    - 7×7 Conv(stride=2) + Max Pool 후, (3,4,6,3) 블록으로 채널(64→128→256→512)을 늘리며 다운샘플
    - 연산량은 3.6G로 VGG 대비 훨씬 적음
- **34-layer Residual(오른쪽)**
    
    - Plain과 동일한 3×3 Conv 수를 가지되, 각 블록마다 skip 연결을 추가
    - 채널이 바뀔 때 점선(dotted) shortcut으로 1×1 Conv 적용
    - 연산량은 Plain과 동일(3.6G)하나, 학습 안정성과 정확도가 향상

위 그림은 이러한 세 가지 네트워크 구조가 어떻게 구성되고, 각 단계에서 출력 맵의 크기와 채널 수가 어떻게 변화하는지, 그리고 대략적인 연산량이 얼마나 다른지를 한눈에 보여준다.

- **숫자는 블록별 출력 크기(예: 56×56)와 채널 수(예: 64)를 의미**
- **네트워크 깊이(레이어 수)는 컨볼루션 레이어와 FC 레이어의 총합**
- **FLOPs 차이**는 채널 수 배치와 다운샘플 방식 차이에서 기인

이렇게 해서 VGG-19, Plain-34, Residual-34 세 가지 모델이 각각 어떤 구조적 특징을 가지며, 왜 FLOPs가 그렇게 표기되는지 이해할 수 있다.