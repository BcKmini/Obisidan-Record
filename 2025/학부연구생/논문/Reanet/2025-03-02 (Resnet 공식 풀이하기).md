![[스크린샷 2025-03-02 15.24.38.png]]


##  Feature map 사이즈가 절반이 되면 filter 수는 2배가 되도록 만드는 이유 -

- 필터 수를 2배로 증가시키는 이유는 feature map의 크기가 절반으로 줄어들기 때문이다. 이미지나 feature map의 크기가 축소되면, 정보를 추출하는 범위가 더 좁아지기 때문에, 더 많은 필터를 사용하여 더 복잡한 패턴을 캡처해야 합니다.
구체적으로, 이미지나 feature map의 크기가 절반이 되면, 필터를 더 많이 적용하여 그 축소된 영역에서 중요한 특징을 추출하도록 해야 합니다. 이렇게 필터 수를 늘리는 방식은 네트워크가 더 많은 정보를 캡처하고, 여러 레벨에서 다양한 특성들을 효과적으로 학습할 수 있도록 돕습니다. 이는 모델의 성능 향상에도 기여할 수 있습니다

### 1. **Degradation 문제 (성능 저하 문제)**

Degradation 문제는 네트워크의 깊이가 깊어질수록 성능이 오히려 떨어지는 현상을 의미합니다. 이는 네트워크가 너무 깊어지면 학습이 제대로 이루어지지 않고, 훈련 오차와 테스트 오차가 모두 증가하는 문제입니다.

#### 원인:

- **Vanishing/Exploding Gradients**: 네트워크가 깊어짐에 따라 역전파 과정에서 기울기가 매우 작거나 매우 커져서 가중치 업데이트가 제대로 이루어지지 않는 현상입니다. 이로 인해 학습이 매우 느리거나 아예 이루어지지 않게 됩니다.
- **정보 손실**: 깊은 네트워크에서는 정보가 여러 층을 거쳐 전달되면서 점차 손실될 수 있습니다. 이로 인해 더 깊은 네트워크가 오히려 더 적은 정보를 처리하게 되어 성능이 떨어질 수 있습니다.
- **훈련이 어려워짐**: 네트워크의 깊이가 커질수록 그만큼 최적화가 어려워집니다. 적절한 초기화나 학습률 조정이 없다면, 성능이 떨어질 가능성이 높습니다.

#### 해결 방법:

- **Residual Networks (ResNet)**: ResNet은 skip connection(잔차 연결)을 도입하여 네트워크의 깊이를 늘려도 성능 저하가 발생하지 않도록 합니다. 각 층에서 전달되는 정보가 손실되지 않고, 기울기 소실 문제를 해결할 수 있습니다.
- **Batch Normalization**: 각 층의 입력을 정규화하여 네트워크의 학습을 안정화시키는 방법입니다. 이 또한 기울기 소실 문제를 일부 해결합니다.

### 2. **오버피팅 (Overfitting)**

오버피팅은 모델이 훈련 데이터에 너무 잘 맞추어져서, 새로운 데이터(테스트 데이터)에서는 성능이 저하되는 현상입니다. 즉, 모델이 훈련 데이터의 노이즈나 작은 세부 사항까지 학습하여 일반화 성능이 떨어지는 것입니다.

#### 원인:

- **모델의 복잡도**: 너무 복잡한 모델(너무 많은 파라미터)을 사용하면 훈련 데이터에 과도하게 적합하여, 새로운 데이터에 대해서는 잘 일반화하지 못합니다.
- **훈련 데이터 부족**: 훈련 데이터가 부족하면 모델이 적절한 패턴을 학습하는 대신, 훈련 데이터에 존재하는 특수한 패턴이나 노이즈에 적합하게 됩니다.
- **정규화 부족**: 정규화 기법이 없으면 모델이 훈련 데이터의 세부 사항에 너무 많이 적합하여 오버피팅이 발생할 수 있습니다.

#### 해결 방법:

- **교차 검증 (Cross-Validation)**: 모델이 테스트 데이터에 대해 일반화될 수 있도록 훈련 데이터를 여러 부분으로 나누어 검증하고, 모델의 성능을 평가합니다.
- **정규화 기법**:
    - **L2 정규화 (Ridge Regression)**: 가중치의 크기를 제한하여 과도하게 큰 가중치가 학습되는 것을 방지합니다.
    - **Dropout**: 학습 중 일부 뉴런을 랜덤하게 무시하여 모델이 특정 뉴런에 의존하는 것을 방지합니다.
- **데이터 증강 (Data Augmentation)**: 데이터 양을 인위적으로 늘려 모델이 다양한 데이터에 대해 학습하도록 유도합니다.

### 3. **Degradation 문제와 오버피팅의 차이점**

- **원인**:
    
    - Degradation 문제는 **네트워크의 깊이가 증가하면서 발생하는 문제**로, 모델이 너무 깊어져서 학습이 제대로 이루어지지 않거나 기울기 소실 문제가 발생하는 것과 관련이 있습니다.
    - 오버피팅은 **모델이 훈련 데이터에 지나치게 적합되어 일반화 성능이 떨어지는 문제**입니다. 이는 모델이 훈련 데이터의 세부 사항이나 노이즈까지 학습해서 발생합니다.
- **결과**:
    
    - Degradation 문제는 **훈련 데이터와 테스트 데이터에서 모두 성능이 저하되는 현상**입니다. 네트워크가 깊어지면, 훈련 오차와 테스트 오차가 동시에 증가하는 경향이 있습니다.
    - 오버피팅은 **훈련 데이터에서 성능은 좋지만 테스트 데이터에서 성능이 나빠지는 현상**입니다. 훈련 데이터에 적합한 모델이 새로운 데이터에 대해서는 잘 작동하지 않습니다.
- **해결 방법**:
    
    - Degradation 문제는 **네트워크의 깊이를 늘리거나 학습 과정을 안정화시키기 위한 기법**(ResNet, Batch Normalization 등)을 사용하여 해결합니다.
    - 오버피팅은 **모델의 복잡도를 낮추거나 정규화 기법을 사용**하여 해결합니다.

---

Identity Mapping by Sortcuts
$$
y = F(x, {W_i}) + x
$$
- 함수 $$
F(x, W_ii)
$$
residual mapping을 나타냄




---

## **왜 conv layer는 3x3을 주로 사용할까 ?**


VGGNet 



---


### 참고
https://nittaku.tistory.com/266
https://jxnjxn.tistory.com/22