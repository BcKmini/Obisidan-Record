## **ResNet**(Residual Networks)이라는 딥러닝 모델을 제안하고, 이를 통해 **이미지 인식 성능을 크게 향상시키는 방법**을 설명 (수식 포함)

### 연구의 계기:

- 기존의 딥러닝 네트워크는 **계층이 깊어지면 성능이 떨어지거나 훈련이 어려워지는 문제**
- 이 문제는 **기울기 소실(vanishing gradient)** 문제 때문인데, 네트워크가 깊어질수록 기울기가 작아져서 가중치 업데이트가 제대로 되지 않아 훈련이 힘듬
-> ResNet은 이러한 문제를 해결하기 위해 **Residual Learning**이라는 방법을 도입 
이는 네트워크에 **skip connection(건너뛰기 연결)**을 추가하여, 네트워크가 깊어져도 성능이 떨어지지 않도록 하는 방식

### 핵심 아이디어:
ResNet의 핵심은 **Residual Block**.  기존의 신경망에서는 입력이 네트워크를 지나면서 여러 연산을 거쳐 출력이 나온다. 하지만 ResNet에서는 출력이 **입력과 더해진** 형태로 나온다.
- 즉, **입력값**을 네트워크에서 계산한 **출력값에 더하는 구조** 
- 이렇게 하면 네트워크가 더 깊어져도 훈련이 잘 이루어질 수 있고, 성능도 좋아진다.

### ResNet 구조:
ResNet은 여러 개의 **Residual Block**을 쌓아 구성된다. 각 **Residual Block**은 다음과 같은 구조를 가지고 있습니다:
1. **입력 x**: 각 블록의 입력.
2. **주 경로**: 일반적인 CNN 경로 (합성곱, 배치 정규화, 활성화 함수 등).
3. **스킵 경로**: 입력을 그대로 통과시켜 주는 경로.
4. **출력 y**: 주 경로의 출력과 스킵 경로의 출력을 더한 결과.

$$
y=F(x,{Wi​})+x
$$

- F(x,{Wi​}): 주 경로에서의 연산, 합성곱 계층, 활성화 함수, 배치 정규화 등이 포함
- x: 스킵 경로에서 입력 값.
- Wi​: 가중치 매개변수들.
- +: 주 경로와 스킵 경로의 출력을 더함.
### 예시:
만약 F(x)F(x)F(x)가 두 개의 합성곱 층이 있는 경우:
$$
F(x)=W2​⋅ReLU(W1​⋅x+b1​)+b2​
$$
그 후, 최종 출력 yyy는 다음과 같이 계산됩니다:
$$
y=F(x)+x
$$

이러한 구조를 사용하면, **Gradient Vanishing/Exploding Problem**을 완화시키고, 더 깊은 네트워크를 효과적으로 학습할 수 있게 된다. ResNet은 이러한 "Residual Block"을 여러 번 쌓아서 모델을 구성한다.


---

![[스크린샷 2025-03-02 15.24.38.png]]


##  Feature map 사이즈가 절반이 되면 filter 수는 2배가 되도록 만드는 이유 -

- 필터 수를 2배로 증가시키는 이유는 feature map의 크기가 절반으로 줄어들기 때문이다. 이미지나 feature map의 크기가 축소되면, 정보를 추출하는 범위가 더 좁아지기 때문에, 더 많은 필터를 사용하여 더 복잡한 패턴을 캡처해야 한다.

> 구체적으로, 이미지나 feature map의 크기가 절반이 되면, 필터를 더 많이 적용하여 그 축소된 영역에서 중요한 특징을 추출하도록 해야 한다. 이렇게 필터 수를 늘리는 방식은 네트워크가 더 많은 정보를 캡처하고, 여러 레벨에서 다양한 특성들을 효과적으로 학습한다. 

---
## 📌 궁금한점
### 1. **Degradation 문제 (성능 저하 문제)**
- Degradation 문제는 네트워크의 깊이가 깊어질수록 성능이 오히려 떨어지는 현상 
- 네트워크가 너무 깊어지면 학습이 제대로 이루어지지 않고, 훈련 오차와 테스트 오차가 모두 증가하는 문제

#### 원인:
- **Vanishing/Exploding Gradients**: 네트워크가 깊어짐에 따라 역전파 과정에서 기울기가 매우 작거나 매우 커져서 가중치 업데이트가 제대로 이루어지지 않는 현상
- 이로 인해 학습이 매우 느리거나 아예 이루어지지 않을 수 있다.
- **정보 손실**: 깊은 네트워크에서는 정보가 여러 층을 거쳐 전달되면서 점차 손실될 수 있습니다. 이로 인해 더 깊은 네트워크가 오히려 더 적은 정보를 처리하게 되어 성능이 떨어질 수 있다.
- **훈련이 어려워짐**: 네트워크의 깊이가 커질수록 그만큼 최적화가 어려워진다. 적절한 초기화나 학습률 조정이 없다면, 성능이 떨어질 가능성이 높다.

#### 해결 방법:
- **Residual Networks (ResNet)**: ResNet은 skip connection(잔차 연결)을 도입하여 네트워크의 깊이를 늘려도 성능 저하가 발생하지 않도록 합니다. 각 층에서 전달되는 정보가 손실되지 않고, 기울기 소실 문제를 해결할 수 있다.
- **Batch Normalization**: 각 층의 입력을 정규화하여 네트워크의 학습을 안정화시키는 방법

---
### 2. **오버피팅 (Overfitting)**
오버피팅은 모델이 훈련 데이터에 너무 잘 맞추어져서, 새로운 데이터(테스트 데이터)에서는 성능이 저하되는 현상이다. 즉, 모델이 훈련 데이터의 노이즈나 작은 세부 사항까지 학습하여 일반화 성능이 떨어지는 것이다.

#### 원인:
- **모델의 복잡도**: 너무 복잡한 모델(너무 많은 파라미터)을 사용하면 훈련 데이터에 과도하게 적합하여, 새로운 데이터에 대해서는 잘 일반화하지 못한다.
- **훈련 데이터 부족**: 훈련 데이터가 부족하면 모델이 적절한 패턴을 학습하는 대신, 훈련 데이터에 존재하는 특수한 패턴이나 노이즈에 적합하게 된다.
- **정규화 부족**: 정규화 기법이 없으면 모델이 훈련 데이터의 세부 사항에 너무 많이 적합하여 오버피팅이 발생할 수 있다.

#### 해결 방법:
- **교차 검증 (Cross-Validation)**: 모델이 테스트 데이터에 대해 일반화될 수 있도록 훈련 데이터를 여러 부분으로 나누어 검증하고, 모델의 성능을 평가
- **정규화 기법**:
    - **L2 정규화 (Ridge Regression)**: 가중치의 크기를 제한하여 과도하게 큰 가중치가 학습되는 것을 방지.
    - **Dropout**: 학습 중 일부 뉴런을 랜덤하게 무시하여 모델이 특정 뉴런에 의존하는 것을 방지.
- **데이터 증강 (Data Augmentation)**: 데이터 양을 인위적으로 늘려 모델이 다양한 데이터에 대해 학습하도록 유도.

### 📌  정리
- Degradation 문제는 **네트워크의 깊이가 증가하면서 발생하는 문제**로, 모델이 너무 깊어져서 학습이 제대로 이루어지지 않거나 기울기 소실 문제가 발생하는 것과 관련있다.
- 오버피팅은 **모델이 훈련 데이터에 지나치게 적합되어 일반화 성능이 떨어지는 문제**, 이는 모델이 훈련 데이터의 세부 사항이나 노이즈까지 학습해서 발생한다.

- Degradation 문제는 **훈련 데이터와 테스트 데이터에서 모두 성능이 저하되는 현상** 네트워크가 깊어지면, 훈련 오차와 테스트 오차가 동시에 증가한다.
- 오버피팅은 **훈련 데이터에서 성능은 좋지만 테스트 데이터에서 성능이 나빠지는 현상**, 훈련 데이터에 적합한 모델이 새로운 데이터에 대해서는 잘 작동하지 않는다.

---

## 📌 궁금한점 
## **왜 conv layer는 3x3을 주로 사용할까 ?**

### 1. **공통점: 3x3 커널 사용**
>  VGGNet과 ResNet 모두 **3x3 커널을 주로 사용**

- VGGNet에서는 **모든 Conv Layer에서 3x3 커널을 일관되게 사용**
- ResNet도 **주요 구조에서 3x3 커널을 사용**
두 네트워크 모두 3x3 커널을 선택한 이유는 **효율성**과 **성능**을 고려한 것이다.

- **VGGNet**: VGGNet은 모든 **Conv Layer에서 3x3 필터를 사용**하여, 작은 필터들이 여러 층에 걸쳐 쌓이도록 설계
- 이는 학습을 더 깊고 복잡하게 만들어서 성능을 개선하고, 5x5나 7x7 필터보다 적은 파라미터와 계산량으로도 비슷한 효과를 낼 수 있었다.
    
- **ResNet**: ResNet은 **Residual Connection**(잔차 연결)을 도입한 모델로, 각 블록은 3x3 커널을 두 번 사용. 
- ResNet은 VGG처럼 **단일 3x3 커널만을 사용**하는 것에 초점을 맞추면서도, **skip connection을 통해 깊이를 깊게 만들 수 있는 장점**을 제공
    

### 2. **차이점: 네트워크 구조**
- **VGGNet**:
    - VGG는 **고정된 깊이의 네트워크**(예: VGG16, VGG19)로, 네트워크 깊이를 늘리기 위해서 여러 층에 걸쳐 동일한 크기의 3x3 커널을 사용
    - VGG의 특징은 **단순한 구조**,  각 블록은 3x3 커널을 여러 개 쌓은 형태로 구성되어 있으며, 이로 인해 **상대적으로 많은 파라미터**를 가진다.
- **ResNet**:
    - ResNet은 **잔차 연결(residual connections)**을 이용해 **매우 깊은 네트워크**를 구축할 수 있다. 이 덕분에, 깊이가 깊어져도 **기울기 소실 문제를 해결**하고, **학습이 효율적으로 이루어지도록** 한다.
    - 각 **residual block** 내에서 3x3 커널을 두 번 사용하며, 각 블록 사이에 **skip connection**을 두어 네트워크가 깊어져도 **성능 저하**를 막을 수 있도록 설계.

### 3. **효율성 및 연산 비용**
- **VGGNet**:
    - VGG는 **필터 크기와 네트워크 깊이**를 증가시키면서 모델을 확장, 그러나 **파라미터 수가 많고 연산량이 많기 때문에** 계산적으로 **비효율적**일 수 있다.
    - 특히 VGG는 **매우 깊은 네트워크**일 경우, **과적합(overfitting)**이 발생할 위험이 크고, 모델이 **더 많은 메모리**를 요구하게 된다.
- **ResNet**:
    - ResNet은 **Residual Block**을 도입하여 모델의 깊이를 늘리면서도 **연산량을 줄이고 학습 효율성을 높일 수 있는 구조**를 가짐, VGG처럼 깊이를 늘리되, **skip connection**을 통해 **매우 깊은 네트워크**를 학습할 수 있다.
    - ResNet은 깊이가 깊어져도 **기울기 소실 문제**가 발생하지 않기 때문에 **효율적인 학습**이 가능하다.

### 4. **주요 차이점: Residual Connection과 학습 효율**
- **VGGNet**은 각 층을 **직렬로 쌓는 방식**이며, 깊이를 늘리면 기울기 소실(gradient vanishing) 문제가 발생할 수 있다. 이로 인해 **학습 속도가 느려지거나, 성능이 저하될 수 있다**.
- **ResNet**은 **잔차 연결**을 사용하여 **매우 깊은 네트워크**에서도 **효율적인 학습**을 가능하게 한다. 이를 통해 **매우 깊은 네트워크**에서도 **기울기 소실** 문제를 해결하고, 학습이 잘 이루어지도록 보장한다.


### 결론: **ResNet과 VGG의 공통점과 차이점**
- **공통점**:
    - 두 모델 모두 **3x3 커널**을 사용하여 효율적인 특성 추출을 한다.
    - 둘 다 **컨볼루션 기반**의 네트워크 구조로, 여러 층을 거쳐 이미지에서 특징을 추출
- **차이점**:
    - VGGNet은 **단순한 직렬 구조**로, 깊이를 늘리면서 **과적합**이 발생할 수 있고, **기울기 소실 문제**가 발생할 수 있다.
    - ResNet은 **Residual Block**을 도입하여 **매우 깊은 네트워크**에서 학습이 효과적으로 이루어지며, **기울기 소실 문제를 해결**하고, **학습 효율성을 높였다**.

---
#### tmux -> 서버 돌리기
```bash
tmux new-session -s mysession
python3 resnet.py
```

- 결과 정리 (표) ressults_df


---
### 참고
https://nittaku.tistory.com/266 (Conv layer 3x3 filter)
https://jxnjxn.tistory.com/ (논문 리뷰)
https://velog.io/@cualquier/220705TIL-Learning-rate-Weight-Initialization-Regularization (학습률, 가중치 초기화)
https://deepinsight.tistory.com/114 (가중치)
